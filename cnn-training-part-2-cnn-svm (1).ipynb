{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\nimport os\nfrom tqdm import tqdm\nimport json\nimport warnings\nimport seaborn as sns\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2023-04-27T07:03:13.130397Z","iopub.execute_input":"2023-04-27T07:03:13.130978Z","iopub.status.idle":"2023-04-27T07:03:14.255631Z","shell.execute_reply.started":"2023-04-27T07:03:13.130903Z","shell.execute_reply":"2023-04-27T07:03:14.254337Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_json = '../input/region-proposals-of-crop-weed-dataset/train.json'\ntest_json = '../input/region-proposals-of-crop-weed-dataset/test.json'\nimages_path = '../input/crop-and-weed-detection-data-with-bounding-boxes/agri_data/data/'\nmodel_path  = '../input/rcnn-training-part-1-finetuning/RCNN_crop_weed_classification_model.h5'\nlabel_csv = '../input/convert-yolo-labels-to-pascalvoc-format/pascal_voc_format.csv'\nnegative_ex_path = '../input/rcnn-data-preprocessing-part-2/Train/background/'","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:03:14.258546Z","iopub.execute_input":"2023-04-27T07:03:14.259175Z","iopub.status.idle":"2023-04-27T07:03:14.266266Z","shell.execute_reply.started":"2023-04-27T07:03:14.259033Z","shell.execute_reply":"2023-04-27T07:03:14.264299Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"with open(train_json,'r') as train:\n    train_region = json.load(train)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:03:14.268151Z","iopub.execute_input":"2023-04-27T07:03:14.268501Z","iopub.status.idle":"2023-04-27T07:03:14.346557Z","shell.execute_reply.started":"2023-04-27T07:03:14.268468Z","shell.execute_reply":"2023-04-27T07:03:14.345199Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"with open(test_json,'r') as test:\n    test_region = json.load(test)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:03:14.348332Z","iopub.execute_input":"2023-04-27T07:03:14.348667Z","iopub.status.idle":"2023-04-27T07:03:14.524997Z","shell.execute_reply.started":"2023-04-27T07:03:14.348635Z","shell.execute_reply":"2023-04-27T07:03:14.523562Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_images_list = list(train_region.keys())\ntest_images_list = list(test_region.keys())","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:03:14.528585Z","iopub.execute_input":"2023-04-27T07:03:14.528946Z","iopub.status.idle":"2023-04-27T07:03:14.535411Z","shell.execute_reply.started":"2023-04-27T07:03:14.528896Z","shell.execute_reply":"2023-04-27T07:03:14.534018Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(len(train_images_list))\nprint(len(test_images_list))","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:03:14.538996Z","iopub.execute_input":"2023-04-27T07:03:14.539620Z","iopub.status.idle":"2023-04-27T07:03:14.549867Z","shell.execute_reply.started":"2023-04-27T07:03:14.539553Z","shell.execute_reply":"2023-04-27T07:03:14.548103Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"1000\n300\n","output_type":"stream"}]},{"cell_type":"code","source":"labels = pd.read_csv(label_csv)\nlabels.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:03:14.551874Z","iopub.execute_input":"2023-04-27T07:03:14.552403Z","iopub.status.idle":"2023-04-27T07:03:14.593228Z","shell.execute_reply.started":"2023-04-27T07:03:14.552348Z","shell.execute_reply":"2023-04-27T07:03:14.592061Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"           filename  width  height class  xmin  ymin  xmax  ymax\n0  agri_0_9354.jpeg    512     512  weed    63   120   425   442\n1  agri_0_9354.jpeg    512     512  weed     0     1   180   148\n2  agri_0_7574.jpeg    512     512  crop    95   167   453   469\n3  agri_0_8960.jpeg    512     512  weed    52    76   422   353\n4   agri_0_417.jpeg    512     512  weed     7    75   511   411","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>width</th>\n      <th>height</th>\n      <th>class</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>xmax</th>\n      <th>ymax</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>agri_0_9354.jpeg</td>\n      <td>512</td>\n      <td>512</td>\n      <td>weed</td>\n      <td>63</td>\n      <td>120</td>\n      <td>425</td>\n      <td>442</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>agri_0_9354.jpeg</td>\n      <td>512</td>\n      <td>512</td>\n      <td>weed</td>\n      <td>0</td>\n      <td>1</td>\n      <td>180</td>\n      <td>148</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>agri_0_7574.jpeg</td>\n      <td>512</td>\n      <td>512</td>\n      <td>crop</td>\n      <td>95</td>\n      <td>167</td>\n      <td>453</td>\n      <td>469</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>agri_0_8960.jpeg</td>\n      <td>512</td>\n      <td>512</td>\n      <td>weed</td>\n      <td>52</td>\n      <td>76</td>\n      <td>422</td>\n      <td>353</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>agri_0_417.jpeg</td>\n      <td>512</td>\n      <td>512</td>\n      <td>weed</td>\n      <td>7</td>\n      <td>75</td>\n      <td>511</td>\n      <td>411</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"model = tf.keras.models.load_model(model_path)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:03:14.595249Z","iopub.execute_input":"2023-04-27T07:03:14.595619Z","iopub.status.idle":"2023-04-27T07:03:30.647588Z","shell.execute_reply.started":"2023-04-27T07:03:14.595583Z","shell.execute_reply":"2023-04-27T07:03:30.646405Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:03:30.649372Z","iopub.execute_input":"2023-04-27T07:03:30.649702Z","iopub.status.idle":"2023-04-27T07:03:30.661688Z","shell.execute_reply.started":"2023-04-27T07:03:30.649666Z","shell.execute_reply":"2023-04-27T07:03:30.659872Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\ndense (Dense)                (None, 4096)              102764544 \n_________________________________________________________________\ndropout (Dropout)            (None, 4096)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 4096)              16781312  \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 4096)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 3)                 12291     \n=================================================================\nTotal params: 134,272,835\nTrainable params: 119,558,147\nNon-trainable params: 14,714,688\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model_without_last_2FC = tf.keras.models.Model(model.inputs,model.layers[-5].output)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:03:30.663604Z","iopub.execute_input":"2023-04-27T07:03:30.663968Z","iopub.status.idle":"2023-04-27T07:03:30.679974Z","shell.execute_reply.started":"2023-04-27T07:03:30.663906Z","shell.execute_reply":"2023-04-27T07:03:30.678601Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model_without_last_2FC.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:03:30.681660Z","iopub.execute_input":"2023-04-27T07:03:30.682102Z","iopub.status.idle":"2023-04-27T07:03:30.697910Z","shell.execute_reply.started":"2023-04-27T07:03:30.682052Z","shell.execute_reply":"2023-04-27T07:03:30.697019Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\ndense (Dense)                (None, 4096)              102764544 \n=================================================================\nTotal params: 117,479,232\nTrainable params: 102,764,544\nNon-trainable params: 14,714,688\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"When we pass image from model it will return (1,4096) size feature vector","metadata":{}},{"cell_type":"code","source":"train_features = []\n\ntest_features = []\n\n\nfor index in tqdm(range(len(labels))):\n    id = labels.loc[index,'filename']\n    img = cv2.imread(images_path + id)\n    rgb_img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    xmin,ymin,xmax,ymax = int(labels.loc[index,'xmin']) ,int(labels.loc[index,'ymin']),int(labels.loc[index,'xmax']),int(labels.loc[index,'ymax'])\n\n    resized = cv2.resize(rgb_img[ymin:ymax,xmin:xmax,:],(224,224))\n\n    feature_of_img = model_without_last_2FC.predict(resized.reshape(1,224,224,3)/255)\n    \n    if id in train_images_list:\n        \n        train_features.append([feature_of_img,labels.loc[index,'class']])\n        \n    else:\n        test_features.append([feature_of_img,labels.loc[index,'class']])\n      ","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:03:30.699852Z","iopub.execute_input":"2023-04-27T07:03:30.700210Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":" 94%|█████████▍| 1956/2072 [11:29<00:41,  2.76it/s]","output_type":"stream"}]},{"cell_type":"code","source":"print(len(train_features))\n\nprint(len(test_features))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index,img in tqdm(enumerate(os.listdir(negative_ex_path)[:5000])):  #only extracting for 10,000 images\n    img = cv2.imread(negative_ex_path + img )\n    rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    #images already in (224,224,3)\n    feature_of_img = model_without_last_2FC.predict(rgb.reshape(1,224,224,3)/255)\n    if index<3500:\n        train_features.append([feature_of_img,'background'])\n    else:\n        test_features.append([feature_of_img,'background'])","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"1715it [09:58,  2.86it/s]","output_type":"stream"}]},{"cell_type":"code","source":"import random\nrandom.shuffle(train_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.array([x[0] for x in train_features])\nX_train = X_train.reshape(-1,4096)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = [x[1] for x in train_features]\ny_train = np.array(y_train).reshape(-1,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = np.array([x[0] for x in test_features])\nX_test = X_test.reshape(-1,4096)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = [x[1] for x in test_features]\ny_test = np.array(y_test).reshape(-1,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_model_linear = SVC(kernel = 'linear', C = 1,probability=True).fit(X_train, y_train) \nsvm_predictions = svm_model_linear.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = svm_model_linear.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, svm_predictions) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(cm,annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = cv2.imread(negative_ex_path + os.listdir(negative_ex_path)[45] )\nrgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.imshow(rgb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_of_img = model_without_last_2FC.predict(rgb.reshape(1,224,224,3)/255)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_model_linear.predict(feature_of_img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_model_linear.predict_proba(feature_of_img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_model_linear.classes_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = cv2.imread(images_path+'agri_0_1024.jpeg')\nrgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.imshow(rgb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resized = cv2.resize(rgb,(224,224))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(resized)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_model_linear.predict_proba(model_without_last_2FC.predict(resized.reshape(1,224,224,3)/255))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open('svm_classifier.pkl','wb') as svm_model:\n    pickle.dump(svm_model_linear , svm_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}